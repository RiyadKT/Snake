{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code provides a neural network (Supervised learning / Q-Network) solution of the snake game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake_functions import game,draw,next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful for the supervised learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake_resol import find_shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m=8,8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefining some functions from 'snake_functions' to gain info for the qnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game class representing the Snake game state\n",
    "class Game:\n",
    "    # Constructor method that initializes the game attributes\n",
    "    def __init__(self, tab, dir, snake_list, score):\n",
    "        self.tab = tab  # The game grid (table) where the snake and food are placed\n",
    "        self.dir = dir  # The current direction of the snake (d1, d2)\n",
    "        self.snake_list = snake_list  # List of coordinates representing the snake's body\n",
    "        self.score = score  # The player's current score\n",
    "\n",
    "    # Method that updates the game state (snake movement, food generation, collision detection)\n",
    "    def update(self):\n",
    "        d1, d2 = self.dir  # The direction of the snake (change in x and y)\n",
    "        current = list.copy(self.snake_list)  # Make a copy of the snake to track its previous state\n",
    "        reward = -10  # Default reward (negative for game over, increased when food is eaten)\n",
    "\n",
    "        # Check if there's food on the grid, and if not, place new food randomly\n",
    "        if not any(\"F\" in self.tab[i] for i in range(n)):\n",
    "            f1, f2 = random.randint(0, n-1), random.randint(0, m-1)\n",
    "            # Ensure that the food is not placed on the snake\n",
    "            while (f1, f2) in self.snake_list:\n",
    "                f1, f2 = random.randint(0, n-1), random.randint(0, m-1)\n",
    "            self.tab[f1, f2] = \"F\"  # Place food ('F') on the grid\n",
    "\n",
    "        # Loop through each segment of the snake\n",
    "        for i in range(len(self.snake_list)):\n",
    "            x, y = self.snake_list[i]  # Get current snake segment's coordinates\n",
    "\n",
    "            if i == 0:  # Head of the snake (first segment)\n",
    "                xf, yf = next_position(x, y, d1, d2)  # Get the next position of the head based on the direction\n",
    "\n",
    "                # Check for collision with snake body or wall\n",
    "                if self.tab[xf, yf] == \"S\" or (x + d1 not in range(n) or y + d2 not in range(m)):\n",
    "                    print(\"Game Over! Score: \" + str(self.score))  # Game over if collision occurs\n",
    "                    return True, -10  # Return game over flag and penalty reward\n",
    "\n",
    "                elif self.tab[xf, yf] == \"F\":  # Check if the head eats food\n",
    "                    a, b = self.snake_list[-1]  # Get the last segment of the snake (tail)\n",
    "                    # Extend the snake by adding a new segment at the tail's previous position\n",
    "                    self.snake_list.append(next_position(a, b, -d1, -d2))\n",
    "                    # Place new food randomly after eating\n",
    "                    f1, f2 = random.randint(0, n-1), random.randint(0, m-1)\n",
    "                    while (f1, f2) in self.snake_list:\n",
    "                        f1, f2 = random.randint(0, n-1), random.randint(0, m-1)\n",
    "                    self.tab[f1, f2] = \"F\"\n",
    "                    self.score += 10  # Increase the score for eating food\n",
    "                    reward = 10  # Reward for eating food\n",
    "\n",
    "                # Update the snake's head position\n",
    "                xn, yn = next_position(x, y, d1, d2)\n",
    "                self.snake_list[i] = xn, yn  # Move the head to the new position\n",
    "                self.tab[xn, yn] = \"S\"  # Mark the new head position on the grid\n",
    "                self.tab[x, y] = \"X\"  # Mark the previous head position as visited\n",
    "\n",
    "                # Loop through the grid to find the food and assign a slight penalty (-0.1) for not eating it yet\n",
    "                for k in range(n):\n",
    "                    for l in range(m):\n",
    "                        if self.tab[k, l] == \"F\":\n",
    "                            f1, f2 = k, l\n",
    "                            reward = -0.1\n",
    "\n",
    "            else:  # Body of the snake (all other segments)\n",
    "                xprev, yprev = current[i - 1]  # Move each body segment to the position of the segment ahead\n",
    "                self.snake_list[i] = xprev, yprev\n",
    "                self.tab[xprev, yprev] = \"S\"  # Mark the new body segment position\n",
    "                self.tab[x, y] = \"X\"  # Mark the previous body segment position as visited\n",
    "\n",
    "        return False, reward  # Return no game over and the current reward\n",
    "\n",
    "# Function to calculate the next position of the snake based on the direction\n",
    "def next_position(x, y, d1, d2):\n",
    "    xf, yf = x, y  # Current coordinates\n",
    "    # Handle grid wrapping if the snake goes off the edge (teleport to opposite side)\n",
    "    if x + d1 == n:\n",
    "        xf = 0\n",
    "    elif x + d1 == -1:\n",
    "        xf = n - 1\n",
    "    if y + d2 == m:\n",
    "        yf = 0\n",
    "    elif y + d2 == -1:\n",
    "        yf = m - 1\n",
    "    # Update the coordinates based on direction\n",
    "    if xf == x:\n",
    "        xf += d1\n",
    "    if yf == y:\n",
    "        yf += d2\n",
    "    return xf, yf  # Return the next position of the snake\n",
    "\n",
    "# Function to draw the game on the screen using pygame\n",
    "def draw(game):\n",
    "    SURF.fill(gris_clair)  # Fill the screen with a light gray background\n",
    "\n",
    "    # Loop through the grid to draw snake, food, and empty cells\n",
    "    for j in range(m):\n",
    "        for i in range(n):\n",
    "            if game.tab[i, j] == \"S\":  # Draw the snake\n",
    "                pg.draw.rect(SURF, noir, [x0 + i * (x1 - x0) / (n - 1), y0 + (m - 1 - j) * (y1 - y0) / (m - 1), (x1 - x0) / n, (y1 - y0) / m])\n",
    "            elif game.tab[i, j] == \"F\":  # Draw the food\n",
    "                pg.draw.rect(SURF, rouge, [x0 + i * (x1 - x0) / (n - 1), y0 + (m - 1 - j) * (y1 - y0) / (m - 1), (x1 - x0) / n, (y1 - y0) / m])\n",
    "            else:  # Draw empty cells\n",
    "                pg.draw.rect(SURF, gris_fonce, [x0 + i * (x1 - x0) / (n - 1), y0 + (m - 1 - j) * (y1 - y0) / (m - 1), (x1 - x0) / n, (y1 - y0) / m])\n",
    "\n",
    "    # Render and display the score on the screen\n",
    "    img = font.render(\"Score: \" + str(game.score), True, noir)\n",
    "    SURF.blit(img, (900, 500))  # Display the score at the specified position\n",
    "    pg.display.update()  # Update the display with the new drawing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pygame as pg  # Library used for creating the game visuals and handling input\n",
    "import random  # Standard library for generating random numbers\n",
    "import numpy as np  # Library for numerical computations (arrays, matrices)\n",
    "import torch  # PyTorch library for building neural networks and handling tensors\n",
    "import torch.nn as nn  # Submodule for creating neural network layers and models\n",
    "import torch.optim as optim  # Submodule for optimization algorithms like Adam\n",
    "from collections import deque  # Data structure for efficient memory storage (double-ended queue)\n",
    "import sys  # Standard library used for system-specific parameters and functions\n",
    "\n",
    "# Game constants\n",
    "#n, m = 10, 10  # Size of the game grid (n x m cells)\n",
    "\n",
    "# Initialize Pygame\n",
    "pg.init()  # Initialize all the imported pygame modules\n",
    "SURF = pg.display.set_mode((1450, 1000))  # Set up the game window with resolution 1450x1000\n",
    "font = pg.font.SysFont(None, 30)  # Initialize the font to display the score\n",
    "\n",
    "# Colors used in the game\n",
    "gris_clair = (220, 220, 220)  # Light gray\n",
    "gris_fonce = (150, 150, 150)  # Dark gray\n",
    "noir = (0, 0, 0)  # Black\n",
    "rouge = (255, 0, 0)  # Red\n",
    "\n",
    "# Initial coordinates for the game area\n",
    "x0, y0 = 200, 150  # Top-left corner of the game grid\n",
    "x1, y1 = 800, 750  # Bottom-right corner of the game grid\n",
    "\n",
    "# Reinforcement Learning (RL) constants\n",
    "GAMMA = 0.95  # Discount factor for future rewards\n",
    "EPSILON = 1.0  # Exploration rate (start with 100% exploration)\n",
    "EPSILON_DECAY = 0.99  # Decay rate of epsilon to gradually favor exploitation\n",
    "MIN_EPSILON = 0.01  # Minimum exploration rate\n",
    "LEARNING_RATE = 0.001  # Learning rate for the optimizer\n",
    "BATCH_SIZE = 1000  # Size of the batch used for training the neural network\n",
    "MEMORY_SIZE = 100_000  # Maximum size of the memory (replay buffer)\n",
    "\n",
    "# DQN (Deep Q-Network) model definition\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        x = 512  # Hidden layer size\n",
    "        self.fc1 = nn.Linear(input_dim, x)  # Input layer connected to the hidden layer\n",
    "        self.fc2 = nn.Linear(x, output_dim)  # Hidden layer connected to the output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation function to the hidden layer\n",
    "        return self.fc2(x)  # Output layer (no activation, as Q-values are raw scores)\n",
    "\n",
    "# RL Agent class that interacts with the environment\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, device='mps'):\n",
    "        self.device = device  # Device (e.g., 'cpu' or 'cuda')\n",
    "        self.state_size = state_size  # Size of the state representation (input to the network)\n",
    "        self.action_size = action_size  # Number of possible actions (output of the network)\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  # Replay buffer to store experiences\n",
    "        self.gamma = GAMMA  # Discount factor for future rewards\n",
    "        self.epsilon = EPSILON  # Initial exploration rate\n",
    "        self.epsilon_min = MIN_EPSILON  # Minimum exploration rate\n",
    "        self.epsilon_decay = EPSILON_DECAY  # Rate at which exploration decays\n",
    "        self.model = DQN(state_size, action_size)  # The Q-network model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)  # Optimizer (Adam)\n",
    "        self.criterion = nn.MSELoss()  # Loss function (Mean Squared Error)\n",
    "\n",
    "    # Method to store an experience in the replay buffer\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))  # Append experience to memory\n",
    "\n",
    "    # Method to choose an action based on the current state (epsilon-greedy policy)\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:  # Explore: choose random action\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)  # Convert state to tensor and add batch dimension\n",
    "        act_values = self.model(state)  # Get Q-values for each action\n",
    "        return torch.argmax(act_values[0]).item()  # Choose action with the highest Q-value\n",
    "\n",
    "    # Method to replay experiences and train the network\n",
    "    def replay(self, method):\n",
    "        if len(self.memory) < BATCH_SIZE:  # If there aren't enough experiences in memory, return\n",
    "            return\n",
    "\n",
    "        # Sample a random batch of experiences from memory\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            # Q-net part\n",
    "            if method == 'unsupervised':\n",
    "                target = reward  # Start with the immediate reward\n",
    "                if not done:  # If the game is not over, calculate future reward\n",
    "                    next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                    target = reward + self.gamma * torch.max(self.model(next_state)[0]).item()\n",
    "                \n",
    "                state = torch.FloatTensor(state).unsqueeze(0)  # Convert state to tensor\n",
    "                target_f = self.model(state)  # Predict Q-values\n",
    "                target_f[0][action] = target  # Update the Q-value for the chosen action\n",
    "\n",
    "                # Perform backpropagation to update the model\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(target_f, self.model(state))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            else: # Supervised learning part\n",
    "                tab_l = state[:n * m]  # Extract the grid from the state\n",
    "                tab_l = np.array(tab_l)\n",
    "                tab = tab_l.reshape((n, m))  # Reshape the flattened grid into n x m\n",
    "\n",
    "                snake_list = []  # List to store snake positions\n",
    "                vals = {}\n",
    "                \n",
    "                # Finding the positions corresponding to the snake/food/snake head\n",
    "                for i in range(n):\n",
    "                    for j in range(m):\n",
    "                        if tab[i, j] != 0:\n",
    "                            if tab[i, j] not in vals.keys():\n",
    "                                vals[tab[i, j]] = [(i, j)]\n",
    "                            else:\n",
    "                                vals[tab[i, j]].append((i, j))\n",
    "                            \n",
    "                vals = dict(sorted(vals.items()))  # Sort snake parts in order\n",
    "                \n",
    "                # Construct the snake based on the IDs\n",
    "                for i, couple in enumerate(vals.items()):\n",
    "                    k, l = couple\n",
    "                    print(i,couple)\n",
    "                    if i == 0:  # Body of the snake\n",
    "                        for x, y in l:\n",
    "                            snake_list.append((x, y))\n",
    "                    elif i == 2:  # Head of the snake\n",
    "                        snake_list.insert(0, l[0])\n",
    "                    elif i == 1:  # Food\n",
    "                        f1, f2 = l[0]\n",
    "\n",
    "                # Placeholder for further logic related to game state and pathfinding\n",
    "                self.optimizer.zero_grad()\n",
    "                x, y = state[n * m:n * m + 3] / np.linalg.norm(state[n * m:n * m + 3])  # Normalize direction\n",
    "                state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "                possible_directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]  # List of possible movement directions\n",
    "                s1, s2 = snake_list[0]  # Get the head of the snake\n",
    "                possible_spots = [(s1 + i, s2 + j) if (s1 + i) in range(n) and (s2 + j) in range(m) and (s1 + i, s2 + j) not in snake_list else (1000, 1000) for (i, j) in possible_directions]  # Determine possible next moves\n",
    "                \n",
    "                # Select the best action based on the distance to the food\n",
    "                action = np.argmin([np.sqrt((i - f1) ** 2 + (j - f2) ** 2) for (i, j) in possible_spots])\n",
    "\n",
    "                actionl = [0 if i != action else 1 for i in range(4)]  # Create one-hot encoding for action\n",
    "                actionl = torch.FloatTensor(actionl)\n",
    "                loss = self.criterion(actionl, self.model(state)[0])  # Calculate loss\n",
    "                loss.backward()  # Perform backpropagation\n",
    "                self.optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay  # Multiply epsilon by decay factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop (a pygame window pops up where you can see training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_numeric(state, snakelist):\n",
    "    \"\"\"Convert the game state (with string elements) to a numerical representation.\n",
    "    \n",
    "    Args:\n",
    "        state: A 2D array representing the game grid with 'S' for the snake, 'F' for food, and 'X' for empty spaces.\n",
    "        snakelist: A list of tuples representing the snake's position on the grid.\n",
    "\n",
    "    Returns:\n",
    "        numeric_state: A 2D array where:\n",
    "            - 0 represents empty space,\n",
    "            - 1 represents the snake's body,\n",
    "            - 2 represents the food,\n",
    "            - 3 represents the snake's head.\n",
    "    \"\"\"\n",
    "    numeric_state = np.zeros((n, m))  # Create an empty grid of zeros\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if state[i, j] == \"S\":  # Snake body is marked as 1\n",
    "                numeric_state[i, j] = 1\n",
    "            elif state[i, j] == \"F\":  # Food is marked as 2\n",
    "                numeric_state[i, j] = 2\n",
    "            elif state[i, j] == \"X\":  # Empty space remains 0\n",
    "                numeric_state[i, j] = 0\n",
    "\n",
    "    # Set the head of the snake to 3 (differentiate from the body)\n",
    "    x, y = snakelist[0]  # Snake's head is the first element in the snake list\n",
    "    numeric_state[x, y] = 3\n",
    "    return numeric_state\n",
    "\n",
    "def numeric_to_state(state):\n",
    "    \"\"\"Convert a numeric grid back to the original string representation.\n",
    "    \n",
    "    Args:\n",
    "        state: A 2D array of numerical values representing the game state.\n",
    "\n",
    "    Returns:\n",
    "        real_state: A 2D array where:\n",
    "            - 'S' represents the snake,\n",
    "            - 'F' represents the food,\n",
    "            - 'X' represents empty space.\n",
    "    \"\"\"\n",
    "    real_state = np.full((n, m), 'X')  # Initialize the grid with 'X' for empty spaces\n",
    "    vals = {}\n",
    "    \n",
    "    # Populate `vals` with the positions of non-zero values\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            if state[i, j] != 0:\n",
    "                if state[i, j] not in vals.keys():\n",
    "                    vals[state[i, j]] = [(i, j)]  # Assign positions to each unique value\n",
    "                else:\n",
    "                    vals[state[i, j]].append((i, j))\n",
    "\n",
    "    vals = dict(sorted(vals.items()))  # Sort values to maintain correct order for snake and food\n",
    "    current = 0\n",
    "    replace_list = ['S', 'F', 'S']  # Replacement order: Snake, Food, Snake\n",
    "    \n",
    "    # Replace numeric values with corresponding characters\n",
    "    for k, l in vals.items():\n",
    "        for i, j in l:\n",
    "            real_state[i, j] = replace_list[current]\n",
    "        current += 1\n",
    "\n",
    "    return real_state\n",
    "\n",
    "# List to store rewards from each episode\n",
    "reward_list = []\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to initialize and run the game with reinforcement learning.\"\"\"\n",
    "    \n",
    "    # Initialize the game state (using your `Game` class)\n",
    "    state_size = n * m + 2  # The state size is the number of cells (n*m) + 2 (for direction vector)\n",
    "    action_size = 4  # 4 possible actions: move up, down, left, or right\n",
    "    agent = Agent(state_size, action_size)  # Create the RL agent\n",
    "    episodes = 100000  # Number of episodes for training\n",
    "\n",
    "    for e in range(episodes):\n",
    "        # Create a new game environment\n",
    "        env_tab = np.full((n, m), \"X\")  # Initialize the game grid with 'X' (empty spaces)\n",
    "        env_dir = (0, 1)  # Snake starts moving to the right\n",
    "        x, y = np.random.randint(0, n-1), np.random.randint(0, m-1)  # Random starting position for snake\n",
    "        d1, d2 = env_dir  # Snake's direction\n",
    "        env_snake = [(x, y), (x - d1, y - d2), (x - 2 * d1, y - 2 * d2)]  # Initialize snake with 3 segments\n",
    "        \n",
    "        # Place food at a random position\n",
    "        f1, f2 = random.randint(0, n-1), random.randint(0, m-1)\n",
    "        env_tab[f1, f2] = \"F\"  # Set food on the grid\n",
    "\n",
    "        score = 0  # Initialize score\n",
    "        game_state = Game(env_tab, env_dir, env_snake, score)  # Initialize the game state\n",
    "\n",
    "        # Convert the current game state to a numerical representation\n",
    "        numeric_state = state_to_numeric(game_state.tab, game_state.snake_list)\n",
    "        state = np.concatenate((numeric_state.reshape(n*m), list(env_dir)))  # Flatten the state and append direction\n",
    "        state = state / (1 if np.linalg.norm(state) == 0 else np.linalg.norm(state))  # Normalize the state vector\n",
    "        done = False  # Flag to indicate if the game is over\n",
    "        reward_epoch = 0  # Initialize the reward for the current episode\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)  # Choose an action using the agent's policy\n",
    "            old_dir = game_state.dir  # Save the old direction\n",
    "\n",
    "            # Update the direction based on the chosen action\n",
    "            if action == 0:  # Turn left\n",
    "                game_state.dir = (1, 0)\n",
    "            elif action == 1:  # Turn right\n",
    "                game_state.dir = (0, 1)\n",
    "            elif action == 2:  # Move up\n",
    "                game_state.dir = (-1, 0)\n",
    "            else:  # Move down\n",
    "                game_state.dir = (0, -1)\n",
    "\n",
    "            # Update the game state and get the reward\n",
    "            done, reward = game_state.update()\n",
    "            reward_epoch += reward  # Accumulate reward for the episode\n",
    "            \n",
    "            # Convert the new game state to a numeric representation\n",
    "            numeric_next_state = state_to_numeric(game_state.tab, game_state.snake_list)\n",
    "            next_state = np.concatenate((numeric_next_state.reshape(n*m), list(game_state.dir)))  # Flatten state\n",
    "            next_state = next_state / (1 if np.linalg.norm(next_state) == 0 else np.linalg.norm(next_state))  # Normalize\n",
    "            \n",
    "            # Store the experience in the agent's memory\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            state = next_state  # Update current state\n",
    "\n",
    "            draw(game_state)  # Visualize the game\n",
    "            pg.time.wait(10)  # Add a small delay for smoother rendering\n",
    "\n",
    "            if done:  # If the game is over, break the loop\n",
    "                print(f\"Game over! Final score: {game_state.score}\")\n",
    "                reward_list.append(reward_epoch)  # Record the reward for this episode\n",
    "                break\n",
    "\n",
    "            # Train the agent\n",
    "            if len(agent.memory) < 50_000:  # Initially use supervised learning\n",
    "                print('some', len(agent.memory))\n",
    "                agent.replay('supervised')\n",
    "            else:  # Switch to unsupervised learning after filling the memory\n",
    "                agent.replay('unsupervised')\n",
    "\n",
    "        print(f\"Episode {e+1}/{episodes}, Score: {game_state.score}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(reward_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
